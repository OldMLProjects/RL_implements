{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Actor_critic_pytorch_separate_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/FrwcwTXtj59SJJFxEcxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nerdk312/RL_implements/blob/master/Actor_critic_pytorch_separate_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRAqqWkzMbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD5DQDYnzht0",
        "colab_type": "code",
        "outputId": "f7dfa210-4fef-4f52-a64a-6bbbe3d24546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "gamma = 0.99\n",
        "log_interval = 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkXQyA7c2kUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    '''\n",
        "    Implements actor\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Policy,self).__init__()\n",
        "        self.affine1 = nn.Linear(4,128)\n",
        "\n",
        "        # actor's layer\n",
        "        self.action_head = nn.Linear(128,2)\n",
        "\n",
        "        # action & reward buffer\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self,x):\n",
        "        '''\n",
        "        forward of both actor\n",
        "        '''\n",
        "        x =  F.relu(self.affine1(x))\n",
        "\n",
        "        # actor: choose action to take from state s_t by returning the probability of each action\n",
        "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
        "\n",
        "        # return values for both actor and critic as a tuple of 2 values:\n",
        "        #1. a list with the probability of each ation over the action space\n",
        "        return action_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHSxB1qSA__G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Value(nn.Module):\n",
        "    '''\n",
        "    Implement critic\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Value,self).__init__()\n",
        "        \n",
        "        self.affine1 = nn.Linear(4,128)\n",
        "        self.value_head = nn.Linear(128,1)\n",
        "        \n",
        "        self.state_list = []\n",
        "\n",
        "    def forward(self,x):\n",
        "        x =  F.relu(self.affine1(x))\n",
        "        state_values =  self.value_head(x)\n",
        "\n",
        "        return state_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-b7iqGb4MYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy_model = Policy()\n",
        "policy_optimizer = optim.Adam(policy_model.parameters(), lr = 3e-3)\n",
        "value_model = Value()\n",
        "value_optimizer = optim.Adam(value_model.parameters(), lr = 3e-3)\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acMg0SgX4ZFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs = policy_model(state)\n",
        "    state_value = value_model(state)\n",
        "\n",
        "    # create a categorical distribution over the list of probabilies of actions\n",
        "    m = Categorical(probs)\n",
        "\n",
        "    # and sample an action using the distribution\n",
        "    action = m.sample()\n",
        "\n",
        "    # save to action buffer\n",
        "    policy_model.saved_actions.append(m.log_prob(action))\n",
        "    value_model.state_list.append(state_value)\n",
        "\n",
        "    #model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "\n",
        "    # the action to take (left or right)\n",
        "    return action.item()\n",
        "\n",
        "def finish_episode():\n",
        "    \"\"\"\n",
        "    Training code. Calculates actor and critic loss and performs backprop\n",
        "    \"\"\"\n",
        "    R = 0\n",
        "    saved_actions = policy_model.saved_actions\n",
        "    state_values = value_model.state_list\n",
        "    policy_losses = [] # list to save actor (policy) loss\n",
        "    value_losses = [] # list to save critic ( value) loss\n",
        "    returns = [] # list to save the true values\n",
        "\n",
        "    # calculate the true value using rewards returned from the environment\n",
        "    for r in policy_model.rewards[::-1]:\n",
        "        # Calculate the discounted value\n",
        "        R = r + gamma*R\n",
        "        returns.insert(0,R) # Nawid - This gets the return for each time step\n",
        "\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns -  returns.mean())/ (returns.std() + eps) # Nawid -  Normalise the reward it seems\n",
        "\n",
        "    for log_prob, value, R in zip(saved_actions,state_values, returns):\n",
        "        advantage = R - value.item() # Nawid - Value function is used as the baseline and in this case the Q value is the actual return\n",
        "\n",
        "        # Calculate actor (policy) loss for each time step\n",
        "        policy_losses.append(-log_prob*advantage)\n",
        "        \n",
        "        # Calculate critic (value) loss using L1 smooth loss\n",
        "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R]))) # Nawid-  Difference between the predicted value and the actual value\n",
        "\n",
        "\n",
        "    # sum up all the values of policy losses and value losses\n",
        "    policy_loss =  torch.stack(policy_losses).sum() \n",
        "    # Reset gradients\n",
        "    policy_optimizer.zero_grad()\n",
        "    \n",
        "    # Perform backprop\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "\n",
        "    value_loss = torch.stack(value_losses).sum()\n",
        "    value_optimizer.zero_grad()\n",
        "    value_loss.backward()\n",
        "    value_optimizer.step()\n",
        "\n",
        "    # reset rewards and action buffer\n",
        "    del policy_model.rewards[:]\n",
        "    del policy_model.saved_actions[:]\n",
        "    del value_model.state_list[:]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV2JBXr291XO",
        "colab_type": "code",
        "outputId": "3e353356-3de5-4ba5-b96c-9a52dc07a24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "def main():\n",
        "    running_reward = 10\n",
        "\n",
        "    # run infinitely many_episodes\n",
        "    for i_episode in count(1):\n",
        "\n",
        "        # reset environment and epsiode reward\n",
        "        state = env.reset()\n",
        "        ep_reward = 0 \n",
        "\n",
        "        # for each episode, only run 9999 steps so we dont infinite loop while learning\n",
        "        for t in range(1,10000):\n",
        "\n",
        "            # select action from policy\n",
        "            action =  select_action(state)\n",
        "\n",
        "            # take the action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            policy_model.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Update cumulative reward\n",
        "        running_reward = 0.05 * ep_reward + (1-0.05) * running_reward\n",
        "\n",
        "        # perform backprob\n",
        "        finish_episode()\n",
        "\n",
        "        # log results\n",
        "        if i_episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "        \n",
        "        # check if we have 'solved the cartpole problem\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "        \n",
        "\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 10\tLast reward: 23.00\tAverage reward: 16.12\n",
            "Episode 20\tLast reward: 32.00\tAverage reward: 21.79\n",
            "Episode 30\tLast reward: 36.00\tAverage reward: 28.46\n",
            "Episode 40\tLast reward: 14.00\tAverage reward: 36.26\n",
            "Episode 50\tLast reward: 48.00\tAverage reward: 42.66\n",
            "Episode 60\tLast reward: 51.00\tAverage reward: 42.62\n",
            "Episode 70\tLast reward: 43.00\tAverage reward: 49.72\n",
            "Episode 80\tLast reward: 76.00\tAverage reward: 53.45\n",
            "Episode 90\tLast reward: 84.00\tAverage reward: 57.50\n",
            "Episode 100\tLast reward: 76.00\tAverage reward: 65.00\n",
            "Episode 110\tLast reward: 78.00\tAverage reward: 81.50\n",
            "Episode 120\tLast reward: 97.00\tAverage reward: 86.99\n",
            "Episode 130\tLast reward: 137.00\tAverage reward: 111.90\n",
            "Episode 140\tLast reward: 185.00\tAverage reward: 125.12\n",
            "Episode 150\tLast reward: 185.00\tAverage reward: 142.30\n",
            "Episode 160\tLast reward: 132.00\tAverage reward: 134.58\n",
            "Episode 170\tLast reward: 200.00\tAverage reward: 145.66\n",
            "Episode 180\tLast reward: 180.00\tAverage reward: 155.76\n",
            "Episode 190\tLast reward: 200.00\tAverage reward: 165.98\n",
            "Episode 200\tLast reward: 200.00\tAverage reward: 175.45\n",
            "Episode 210\tLast reward: 128.00\tAverage reward: 176.52\n",
            "Episode 220\tLast reward: 70.00\tAverage reward: 145.93\n",
            "Episode 230\tLast reward: 119.00\tAverage reward: 139.83\n",
            "Episode 240\tLast reward: 200.00\tAverage reward: 150.97\n",
            "Episode 250\tLast reward: 200.00\tAverage reward: 170.06\n",
            "Episode 260\tLast reward: 200.00\tAverage reward: 177.65\n",
            "Episode 270\tLast reward: 200.00\tAverage reward: 180.67\n",
            "Episode 280\tLast reward: 163.00\tAverage reward: 161.68\n",
            "Episode 290\tLast reward: 200.00\tAverage reward: 171.36\n",
            "Episode 300\tLast reward: 200.00\tAverage reward: 182.85\n",
            "Episode 310\tLast reward: 200.00\tAverage reward: 188.93\n",
            "Episode 320\tLast reward: 200.00\tAverage reward: 191.99\n",
            "Episode 330\tLast reward: 200.00\tAverage reward: 194.78\n",
            "Solved! Running reward is now 195.04057889996088 and the last episode runs to 200 time steps!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJF_ToNJWAAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}